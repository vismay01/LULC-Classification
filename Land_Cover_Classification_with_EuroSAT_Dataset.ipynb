{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vismay01/LULC-Classification/blob/main/Land_Cover_Classification_with_EuroSAT_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "nilesh789_eurosat_rgb_path = kagglehub.dataset_download('nilesh789/eurosat-rgb')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "qGncw_arE8bG"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ZyiP5QRlE8bI"
      },
      "cell_type": "markdown",
      "source": [
        "# Land-Cover Classification with EuroSAT Dataset\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/phelber/EuroSAT/master/eurosat_overview_small.jpg)\n",
        "\n",
        "\n",
        "\n",
        "In this notebook we will tackle the problem of land-use and land-cover classification where we try to identify the class of given remote sensing image based on some well defined target class labels. For training the model we will use the EuroSAT dataset from this [paper](https://arxiv.org/abs/1709.00029) published in 2019 which is now a standard benchmark for land-cover classification task.\n",
        "\n",
        "This dataset consisits of 64x64 images captured by Sentinel-2A satellite and it has over **27000 images** spread across **10 classes**. Originally the data consists of hyperspectral images with 13 spectral bands but we will be working with only RGB channels downloaded from [here](http://madm.dfki.de/files/sentinel/EuroSAT.zip).\n",
        "\n",
        "The paper states that the finetuned CNN model **ResNet50** performed best with 80-20 split on the data for RGB channels giving about **98.57%** accuracy.\n",
        "\n",
        "Finally, this notebook was forked and edited from [here](https://www.kaggle.com/silentj23/eurosat-image-classification#II.-Preprocessing).\n",
        "\n",
        "**NOTE:** This notebook is still a work in progress so expect a final version soon"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "TPyCdAGPE8bJ"
      },
      "cell_type": "code",
      "source": [
        "# Importing few libraries\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import PIL\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JNDI4ozlE8bJ"
      },
      "cell_type": "markdown",
      "source": [
        "# I. Data Exploration"
      ]
    },
    {
      "metadata": {
        "id": "y_ZQkl-9E8bK"
      },
      "cell_type": "markdown",
      "source": [
        "In this project, I'll be exploring the EUROSAT dataset. The EUROSAT dataset is composed of images taken from the Sentinel-2 satellite. This dataset lists images of the earth's surface into 10 different land cover labels. For this project, I will build an image classification model for predicting a land cover label, given an image."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-d8i9RpiE8bK"
      },
      "cell_type": "code",
      "source": [
        "DATASET = \"../input/2750\"\n",
        "\n",
        "LABELS = os.listdir(DATASET)\n",
        "print(LABELS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "uMJD-5pWE8bK"
      },
      "cell_type": "code",
      "source": [
        "# plot class distributions of whole dataset\n",
        "counts = {}\n",
        "\n",
        "for l in LABELS:\n",
        "    counts[l] = len(os.listdir(os.path.join(DATASET, l)))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.bar(range(len(counts)), list(counts.values()), align='center')\n",
        "plt.xticks(range(len(counts)), list(counts.keys()), fontsize=12, rotation=40)\n",
        "plt.xlabel('class label', fontsize=13)\n",
        "plt.ylabel('class size', fontsize=13)\n",
        "plt.title('EUROSAT Class Distribution', fontsize=15);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uXH72Pq1E8bK"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset is split into 10 classes of land cover. Each class varies in size, so I'll have to stratify later on when splitting the data into training, testing and validation sets."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lTJ_pEsWE8bL"
      },
      "cell_type": "code",
      "source": [
        "img_paths = [os.path.join(DATASET, l, l+'_1000.jpg') for l in LABELS]\n",
        "\n",
        "img_paths = img_paths + [os.path.join(DATASET, l, l+'_2000.jpg') for l in LABELS]\n",
        "\n",
        "def plot_sat_imgs(paths):\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for i in range(20):\n",
        "        plt.subplot(4, 5, i+1, xticks=[], yticks=[])\n",
        "        img = PIL.Image.open(paths[i], 'r')\n",
        "        plt.imshow(np.asarray(img))\n",
        "        plt.title(paths[i].split('/')[-2])\n",
        "\n",
        "plot_sat_imgs(img_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "keEuuI4oE8bL"
      },
      "cell_type": "markdown",
      "source": [
        "Looking at the preview of the different classes, we can see some similarities and stark differences between the classes.\n",
        "\n",
        "Urban environments such as Highway, Residential and Industrial images all contain structures and some roadways.\n",
        "\n",
        "AnnualCrops and PermanentCrops both feature agricultural land cover, with straight lines dilineating different crop fields.\n",
        "\n",
        "Finally, HerbaceaousVegetation, Pasture, and Forests feature natural land cover; Rivers also could be categorized as natural land cover as well, but may be easier to distinguish from the other natural classes.\n",
        "\n",
        "If we consider the content of each image, we might be able to estimate which classes might be confused for each other. For example, an image of a river might be mistaken for a highway. Or an image of a highway junction, with surrounding buildings, could be mistaken for an Industrial site. We'll have to train a classifier powerful enough to differentiate these nuances.\n",
        "\n",
        "Sentinel-2 satellite images could also be downloaded with 10+ additional bands. Near-Infrared Radiation bands, for example, is a band of data that is available for this dataset. NIR can be used to create an index, visualising the radiation that is present (or not present) in a picture. This dataset does not contain the NIR wavelength bands, so this option will not be explored. But it's worth pointing out that this classification task could be addressed in another way using NIR data."
      ]
    },
    {
      "metadata": {
        "id": "7s_78PkjE8bL"
      },
      "cell_type": "markdown",
      "source": [
        "# II. Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "LyOZmzeIE8bL"
      },
      "cell_type": "markdown",
      "source": [
        "I'd like to evaluate the performance of the model later on after training, so I'll perform a stratified shuffle-split using Scikit-learn to maintain class proportions. 30% of the dataset will be held for evaluation purposes. I'll be loading my data into the Keras model using the ImageDataGenerator class. I'll need the images to be in their own respective land cover directories.\n",
        "\n",
        "After splitting the dataset, I'll create some image augmentations using the generator and also denote a subset of the training data to be used as validation data during training."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tMQDKN6GE8bM"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAIN_DIR = '../working/training'\n",
        "TEST_DIR = '../working/testing'\n",
        "BATCH_SIZE = 64\n",
        "NUM_CLASSES=len(LABELS)\n",
        "INPUT_SHAPE = (64, 64, 3)\n",
        "CLASS_MODE = 'categorical'\n",
        "\n",
        "# create training and testing directories\n",
        "for path in (TRAIN_DIR, TEST_DIR):\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "\n",
        "# create class label subdirectories in train and test\n",
        "for l in LABELS:\n",
        "\n",
        "    if not os.path.exists(os.path.join(TRAIN_DIR, l)):\n",
        "        os.mkdir(os.path.join(TRAIN_DIR, l))\n",
        "\n",
        "    if not os.path.exists(os.path.join(TEST_DIR, l)):\n",
        "        os.mkdir(os.path.join(TEST_DIR, l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gG5mRHKDE8bM"
      },
      "cell_type": "code",
      "source": [
        "# map each image path to their class label in 'data'\n",
        "data = {}\n",
        "\n",
        "for l in LABELS:\n",
        "    for img in os.listdir(DATASET+'/'+l):\n",
        "        data.update({os.path.join(DATASET, l, img): l})\n",
        "\n",
        "X = pd.Series(list(data.keys()))\n",
        "y = pd.get_dummies(pd.Series(data.values()))\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=69)\n",
        "\n",
        "# split the list of image paths\n",
        "for train_idx, test_idx in split.split(X, y):\n",
        "\n",
        "    train_paths = X[train_idx]\n",
        "    test_paths = X[test_idx]\n",
        "\n",
        "    # define a new path for each image depending on training or testing\n",
        "    new_train_paths = [re.sub('\\.\\.\\/input\\/2750', '../working/training', i) for i in train_paths]\n",
        "    new_test_paths = [re.sub('\\.\\.\\/input\\/2750', '../working/testing', i) for i in test_paths]\n",
        "\n",
        "    train_path_map = list((zip(train_paths, new_train_paths)))\n",
        "    test_path_map = list((zip(test_paths, new_test_paths)))\n",
        "\n",
        "    # move the files\n",
        "    print(\"moving training files..\")\n",
        "    for i in tqdm(train_path_map):\n",
        "        if not os.path.exists(i[1]):\n",
        "            if not os.path.exists(re.sub('training', 'testing', i[1])):\n",
        "                shutil.copy(i[0], i[1])\n",
        "\n",
        "    print(\"moving testing files..\")\n",
        "    for i in tqdm(test_path_map):\n",
        "        if not os.path.exists(i[1]):\n",
        "            if not os.path.exists(re.sub('training', 'testing', i[1])):\n",
        "                shutil.copy(i[0], i[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Hxexh-H3E8bM"
      },
      "cell_type": "code",
      "source": [
        "# Create a ImageDataGenerator Instance which can be used for data augmentation\n",
        "\n",
        "train_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=60,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip = True\n",
        "#   validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    directory=TRAIN_DIR,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=CLASS_MODE,\n",
        "    #subset='training',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    seed=69\n",
        ")\n",
        "# The validation set is optional if we choose to do that\n",
        "\"\"\"\n",
        "valid_generator = train_gen.flow_from_directory(\n",
        "    directory=TRAIN_DIR,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=CLASS_MODE,\n",
        "    subset='validation',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    seed=69\n",
        ")\n",
        "\"\"\"\n",
        "# test generator for evaluation purposes with no augmentations, just rescaling\n",
        "test_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        ")\n",
        "\n",
        "test_generator = test_gen.flow_from_directory(\n",
        "    directory=TEST_DIR,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=CLASS_MODE,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    seed=69\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vvTwPcKnE8bM"
      },
      "cell_type": "code",
      "source": [
        "print(train_generator.class_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Ld38VPIbE8bM"
      },
      "cell_type": "code",
      "source": [
        "np.save('class_indices', train_generator.class_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2T5vaMI0E8bM"
      },
      "cell_type": "markdown",
      "source": [
        "# III. Machine Learning for Image Classification\n",
        "\n",
        "First, we will see how a machine learning model performs by directly feeding it the whole image pixels (64x64x3 = 12288). From our 80-20 split on the 27000 samples dataset the training data is of size 21600 and test data is of size 5400. Now, here I will use some trick to utilize keras ImageDataGenerator such that we can obtain the image dataset as a numpy array which can be used by a machine learning model for training and testing.\n",
        "\n",
        "Now, here I will test Random Forest Classifier. First by using the direct implementation provided by scikit-learn and other implemented from scratch. This notebook is a part of my course project because of which I have included this additional implementation from scratch. Feel free to check it out or you can skip over it if you want.\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "RZihFxTZE8bN"
      },
      "cell_type": "code",
      "source": [
        "# Using image data generator api in keras for making image dataset\n",
        "rf_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "rf_train_generator = rf_gen.flow_from_directory(\n",
        "    directory=TRAIN_DIR,\n",
        "    target_size=(64, 64),\n",
        "    # by using batch_size as training data size we can extract data from this iterator\n",
        "    batch_size=21600,\n",
        "    class_mode=CLASS_MODE,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    seed=7\n",
        ")\n",
        "\n",
        "rf_test_generator = rf_gen.flow_from_directory(\n",
        "    directory=TEST_DIR,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=5400,\n",
        "    class_mode=CLASS_MODE,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    seed=7\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Ho7k2d1uE8bN"
      },
      "cell_type": "code",
      "source": [
        "train = rf_train_generator.next()\n",
        "X_train = train[0].reshape(21600, 12288)\n",
        "y_train = train[1]\n",
        "\n",
        "test = rf_test_generator.next()\n",
        "X_test = test[0].reshape(5400, 12288)\n",
        "y_test = test[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZIbgVpTQE8bN"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Random Forest Scikit-Learn Implementation"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ut5LB7VDE8bN"
      },
      "cell_type": "code",
      "source": [
        "# Creating mini batches of training data as training on whole data is difficult\n",
        "ids_1 = np.random.choice(len(X_train), size=21600, replace=False)\n",
        "X_train_mini_1 = X_train[ids_1]\n",
        "y_train_mini_1 = rf_train_generator.classes[ids_1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ggOoIQ16E8bN"
      },
      "cell_type": "code",
      "source": [
        "# import random forest classifier\n",
        "from sklearn import ensemble\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "rf_clf = ensemble.RandomForestClassifier(n_estimators=20, n_jobs=-1, random_state=7)\n",
        "rf_clf.fit(X_train_mini_1, y_train_mini_1)\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "acc = accuracy_score(y_pred, rf_test_generator.classes)\n",
        "print(\"Accuracy Score: {0:.4}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ASK8dx1LE8bN"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Random Forest Implementation from scratch\n",
        "The dataset size is quite large (21600x12288).With scikit-learn's implementation of Random Forest we were barely able to train the model. The implementation from scratch is however not that efficient so we will only provide it with a fraction of training data. Still it can be observed that it produces reasonable accuracy considering for a 10 class classification problem a random guessing approach gives accuracy of 10%"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ao3UA2KjE8bN"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.stats import mode\n",
        "\n",
        "\n",
        "def shuffle_in_unison(a, b):\n",
        "    \"\"\" Shuffles two lists of equal length and keeps corresponding elements in the same index. \"\"\"\n",
        "    rng_state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(rng_state)\n",
        "    np.random.shuffle(b)\n",
        "\n",
        "\n",
        "def entropy(Y):\n",
        "    \"\"\" In information theory, entropy is a measure of the uncertanty of a random sample from a group. \"\"\"\n",
        "\n",
        "    distribution = Counter(Y)\n",
        "    s = 0.0\n",
        "    total = len(Y)\n",
        "    for y, num_y in distribution.items():\n",
        "        probability_y = (num_y/total)\n",
        "        s += (probability_y)*np.log(probability_y)\n",
        "    return -s\n",
        "\n",
        "\n",
        "def information_gain(y, y_true, y_false):\n",
        "    \"\"\" The reduction in entropy from splitting data into two groups. \"\"\"\n",
        "    return entropy(y) - (entropy(y_true)*len(y_true) + entropy(y_false)*len(y_false))/len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AORKmayZE8bN"
      },
      "cell_type": "code",
      "source": [
        "# Implementing a Decision Tree Classifier\n",
        "\n",
        "class DecisionTreeClassifier(object):\n",
        "    \"\"\" A decision tree classifier.\n",
        "    A decision tree is a structure in which each node represents a binary\n",
        "    conditional decision on a specific feature, each branch represents the\n",
        "    outcome of the decision, and each leaf node represents a final\n",
        "    classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_features=lambda x: x, max_depth=10,\n",
        "                    min_samples_split=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_features: A function that controls the number of features to\n",
        "                randomly consider at each split. The argument will be the number\n",
        "                of features in the data.\n",
        "            max_depth: The maximum number of levels the tree can grow downwards\n",
        "                before forcefully becoming a leaf.\n",
        "            min_samples_split: The minimum number of samples needed at a node to\n",
        "                justify a new node split.\n",
        "        \"\"\"\n",
        "\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Builds the tree by chooseing decision rules for each node based on\n",
        "        the data. \"\"\"\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        n_sub_features = int(self.max_features(n_features))\n",
        "        feature_indices = random.sample(range(n_features), n_sub_features)\n",
        "\n",
        "        self.trunk = self.build_tree(X, y, feature_indices, 0)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predict the class of each sample in X. \"\"\"\n",
        "\n",
        "        num_samples = X.shape[0]\n",
        "        y = np.empty(num_samples)\n",
        "        for j in range(num_samples):\n",
        "            node = self.trunk\n",
        "\n",
        "            while isinstance(node, Node):\n",
        "                if X[j][node.feature_index] <= node.threshold:\n",
        "                    node = node.branch_true\n",
        "                else:\n",
        "                    node = node.branch_false\n",
        "            y[j] = node\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "    def build_tree(self, X, y, feature_indices, depth):\n",
        "        \"\"\" Recursivly builds a decision tree. \"\"\"\n",
        "\n",
        "        if depth is self.max_depth or len(y) < self.min_samples_split or entropy(y) is 0:\n",
        "            return mode(y)[0][0]\n",
        "\n",
        "        feature_index, threshold = find_split(X, y, feature_indices)\n",
        "\n",
        "        X_true, y_true, X_false, y_false = split(X, y, feature_index, threshold)\n",
        "        if y_true.shape[0] is 0 or y_false.shape[0] is 0:\n",
        "            return mode(y)[0][0]\n",
        "\n",
        "        branch_true = self.build_tree(X_true, y_true, feature_indices, depth + 1)\n",
        "        branch_false = self.build_tree(X_false, y_false, feature_indices, depth + 1)\n",
        "\n",
        "        return Node(feature_index, threshold, branch_true, branch_false)\n",
        "\n",
        "\n",
        "def find_split(X, y, feature_indices):\n",
        "    \"\"\" Returns the best split rule for a tree node. \"\"\"\n",
        "\n",
        "    num_features = X.shape[1]\n",
        "\n",
        "    best_gain = 0\n",
        "    best_feature_index = 0\n",
        "    best_threshold = 0\n",
        "    for feature_index in feature_indices:\n",
        "        values = sorted(set(X[:, feature_index])) ### better way\n",
        "\n",
        "        for j in range(len(values) - 1):\n",
        "            threshold = (values[j] + values[j+1])/2\n",
        "            X_true, y_true, X_false, y_false = split(X, y, feature_index, threshold)\n",
        "            gain = information_gain(y, y_true, y_false)\n",
        "\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature_index = feature_index\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature_index, best_threshold\n",
        "\n",
        "\n",
        "class Node(object):\n",
        "    \"\"\" A node in a decision tree with the binary condition xi <= t. \"\"\"\n",
        "\n",
        "    def __init__(self, feature_index, threshold, branch_true, branch_false):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.branch_true = branch_true\n",
        "        self.branch_false = branch_false\n",
        "\n",
        "\n",
        "def split(X, y, feature_index, threshold):\n",
        "    \"\"\" Splits X and y based on the binary condition xi <= threshold. \"\"\"\n",
        "\n",
        "    X_true = []\n",
        "    y_true = []\n",
        "    X_false = []\n",
        "    y_false = []\n",
        "\n",
        "    for j in range(len(y)):\n",
        "        if X[j][feature_index] <= threshold:\n",
        "            X_true.append(X[j])\n",
        "            y_true.append(y[j])\n",
        "        else:\n",
        "            X_false.append(X[j])\n",
        "            y_false.append(y[j])\n",
        "\n",
        "    X_true = np.array(X_true)\n",
        "    y_true = np.array(y_true)\n",
        "    X_false = np.array(X_false)\n",
        "    y_false = np.array(y_false)\n",
        "\n",
        "    return X_true, y_true, X_false, y_false"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "edQFaPx2E8bN"
      },
      "cell_type": "code",
      "source": [
        "# Implementing Random Forest Clasifier\n",
        "\n",
        "class RandomForestClassifier(object):\n",
        "    \"\"\" A random forest classifier.\n",
        "    A random forest is a collection of decision trees that vote on a\n",
        "    classification decision. Each tree is trained with a subset of the data and\n",
        "    features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=32, max_features=np.sqrt, max_depth=10,\n",
        "        min_samples_split=2, bootstrap=0.9):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_estimators: The number of decision trees in the forest.\n",
        "            max_features: Controls the number of features to randomly consider\n",
        "                at each split.\n",
        "            max_depth: The maximum number of levels that the tree can grow\n",
        "                downwards before forcefully becoming a leaf.\n",
        "            min_samples_split: The minimum number of samples needed at a node to\n",
        "                justify a new node split.\n",
        "            bootstrap: The fraction of randomly choosen data to fit each tree on.\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.bootstrap = bootstrap\n",
        "        self.forest = []\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Creates a forest of decision trees using a random subset of data and\n",
        "            features. \"\"\"\n",
        "        self.forest = []\n",
        "        n_samples = len(y)\n",
        "        n_sub_samples = round(n_samples*self.bootstrap)\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            shuffle_in_unison(X, y)\n",
        "            X_subset = X[:n_sub_samples]\n",
        "            y_subset = y[:n_sub_samples]\n",
        "\n",
        "            tree = DecisionTreeClassifier(self.max_features, self.max_depth,\n",
        "                                            self.min_samples_split)\n",
        "            tree.fit(X_subset, y_subset)\n",
        "            self.forest.append(tree)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predict the class of each sample in X. \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_trees = len(self.forest)\n",
        "        predictions = np.empty([n_trees, n_samples])\n",
        "        for i in range(n_trees):\n",
        "            predictions[i] = self.forest[i].predict(X)\n",
        "\n",
        "        return mode(predictions)[0][0]\n",
        "\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\" Return the accuracy of the prediction of X compared to y. \"\"\"\n",
        "        y_predict = self.predict(X)\n",
        "        n_samples = len(y)\n",
        "        correct = 0\n",
        "        for i in range(n_samples):\n",
        "            if y_predict[i] == y[i]:\n",
        "                correct = correct + 1\n",
        "        accuracy = correct/n_samples\n",
        "        return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PBSRAQYxE8bO"
      },
      "cell_type": "code",
      "source": [
        "# Creating mini batches of training data as training on whole data is difficult\n",
        "ids_2 = np.random.choice(len(X_train), size=2160, replace=False)\n",
        "X_train_mini_2 = X_train[ids_2]\n",
        "y_train_mini_2 = rf_train_generator.classes[ids_2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gQ14fSa7E8bO"
      },
      "cell_type": "code",
      "source": [
        "forest = RandomForestClassifier(n_estimators=2)\n",
        "forest.fit(X_train_mini_2, y_train_mini_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HUGip3kfE8bO"
      },
      "cell_type": "code",
      "source": [
        "accuracy = forest.score(X_test, rf_test_generator.classes)\n",
        "print(\"Accuracy Score: {0:.4}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WxoORwgxE8bO"
      },
      "cell_type": "markdown",
      "source": [
        "# IV. Deep Learning For Image Classification\n",
        "\n",
        "Deep Learning has highly influenced the field of computer vision when Convolutional Neural Networks (CNN) models were used in tasks like image classification, object detection, facial recognition etc. As discussed by authors of EuroSAT paper many deep learning models outperform the traditional non deep learning methods by a large margin.\n",
        "\n",
        "In this section we will train many state of the art architectures which performed well on the ILSVRC challenge. The authors achieved an accuracy of 98.57% using a fine tuned ResNet50 model. Here, I will try to employ a similar strategy for training the models where initially the CNN part of the model will be frozen with imagenet weights and dense layers will be trained with a high learning rate of 0.01 and later we will train the whole model end-to-end i.e. fine tune by keeping a small learning rate between 0.001 to 0.0001\n",
        "\n",
        "Before we start here is a list of CNN models which we will train:\n",
        "1. ResNet50\n",
        "2. ResNet50V2\n",
        "3. ResNet152V2\n",
        "3. VGG16\n",
        "4. VGG19\n",
        "\n",
        "In future\n",
        "5. InceptionV3\n",
        "6. Xception"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8ivGoCdRE8bO"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from keras.applications import VGG16, VGG19\n",
        "from keras.applications import ResNet50, ResNet50V2, ResNet152V2\n",
        "from keras.applications import InceptionV3, Xception\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, fbeta_score, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ldHvY9ACE8bO"
      },
      "cell_type": "code",
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "\n",
        "tf.config.set_soft_device_placement(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "R9bvb39xE8bO"
      },
      "cell_type": "code",
      "source": [
        "# Note that for different CNN models we are using different setup of dense layers\n",
        "def compile_model(cnn_base, input_shape, n_classes, optimizer, fine_tune=None):\n",
        "\n",
        "    if (cnn_base == 'ResNet50') or (cnn_base == 'ResNet50V2') or (cnn_base == 'ResNet152V2'):\n",
        "        if cnn_base == 'ResNet50':\n",
        "            conv_base = ResNet50(include_top=False,\n",
        "                                 weights='imagenet',\n",
        "                                 input_shape=input_shape)\n",
        "        elif cnn_base == 'ResNet50V2':\n",
        "            conv_base = ResNet50V2(include_top=False,\n",
        "                                 weights='imagenet',\n",
        "                                 input_shape=input_shape)\n",
        "        else:\n",
        "            conv_base = ResNet152V2(include_top=False,\n",
        "                                 weights='imagenet',\n",
        "                                 input_shape=input_shape)\n",
        "        top_model = conv_base.output\n",
        "        top_model = Flatten()(top_model)\n",
        "        top_model = Dense(2048, activation='relu')(top_model)\n",
        "        top_model = Dropout(0.2)(top_model)\n",
        "\n",
        "\n",
        "    elif (cnn_base == 'VGG16') or (cnn_base == 'VGG19'):\n",
        "        if cnn_base == 'VGG16':\n",
        "            conv_base = VGG16(include_top=False,\n",
        "                              weights='imagenet',\n",
        "                              input_shape=input_shape)\n",
        "        else:\n",
        "            conv_base = VGG19(include_top=False,\n",
        "                              weights='imagenet',\n",
        "                              input_shape=input_shape)\n",
        "        top_model = conv_base.output\n",
        "        top_model = Flatten()(top_model)\n",
        "        top_model = Dense(2048, activation='relu')(top_model)\n",
        "        top_model = Dropout(0.2)(top_model)\n",
        "        top_model = Dense(2048, activation='relu')(top_model)\n",
        "        top_model = Dropout(0.2)(top_model)\n",
        "\n",
        "\n",
        "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
        "\n",
        "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
        "\n",
        "    if type(fine_tune) == int:\n",
        "        for layer in conv_base.layers[fine_tune:]:\n",
        "            layer.trainable = True\n",
        "    else:\n",
        "        for layer in conv_base.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
        "                 metrics=['categorical_accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def plot_history(history):\n",
        "\n",
        "    acc = history.history['categorical_accuracy']\n",
        "    val_acc = history.history['val_categorical_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "    plt.show();\n",
        "\n",
        "def display_results(y_true, y_preds, class_labels):\n",
        "\n",
        "    results = pd.DataFrame(precision_recall_fscore_support(y_true, y_preds),\n",
        "                          columns=class_labels).T\n",
        "    results.rename(columns={0: 'Precision',\n",
        "                           1: 'Recall',\n",
        "                           2: 'F-Score',\n",
        "                           3: 'Support'}, inplace=True)\n",
        "\n",
        "    conf_mat = pd.DataFrame(confusion_matrix(y_true, y_preds),\n",
        "                            columns=class_labels,\n",
        "                            index=class_labels)\n",
        "    f2 = fbeta_score(y_true, y_preds, beta=2, average='micro')\n",
        "    accuracy = accuracy_score(y_true, y_preds)\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Global F2 Score: {f2}\")\n",
        "    return results, conf_mat\n",
        "\n",
        "def plot_predictions(y_true, y_preds, test_generator, class_indices):\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    for i, idx in enumerate(np.random.choice(test_generator.samples, size=20, replace=False)):\n",
        "        ax = fig.add_subplot(4, 5, i + 1, xticks=[], yticks=[])\n",
        "        ax.imshow(np.squeeze(test_generator[idx]))\n",
        "        pred_idx = np.argmax(y_preds[idx])\n",
        "        true_idx = y_true[idx]\n",
        "\n",
        "        plt.tight_layout()\n",
        "        ax.set_title(\"{}\\n({})\".format(class_indices[pred_idx], class_indices[true_idx]),\n",
        "                     color=(\"green\" if pred_idx == true_idx else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "sNiQ-Ha-E8bP"
      },
      "cell_type": "code",
      "source": [
        "N_STEPS = train_generator.samples//BATCH_SIZE\n",
        "N_VAL_STEPS = test_generator.samples//BATCH_SIZE\n",
        "N_EPOCHS = 100\n",
        "\n",
        "# model callbacks\n",
        "checkpoint = ModelCheckpoint(filepath='../working/model.weights.best.hdf5',\n",
        "                        monitor='val_categorical_accuracy',\n",
        "                        save_best_only=True,\n",
        "                        verbose=1)\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_categorical_accuracy',\n",
        "                           patience=10,\n",
        "                           restore_best_weights=True,\n",
        "                           mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n",
        "                              patience=3, min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPv-0s_AE8bP"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.1 ResNet50 Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VM-V7foAE8bP"
      },
      "cell_type": "code",
      "source": [
        "resnet50_model = compile_model('ResNet50', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\n",
        "resnet50_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KZykTEqvE8bP"
      },
      "cell_type": "code",
      "source": [
        "train_generator.reset()\n",
        "test_generator.reset()\n",
        "\n",
        "N_STEPS = train_generator.samples//BATCH_SIZE\n",
        "N_VAL_STEPS = test_generator.samples//BATCH_SIZE\n",
        "N_EPOCHS = 100\n",
        "\n",
        "# model callbacks\n",
        "checkpoint = ModelCheckpoint(filepath='../working/model.weights.best.hdf5',\n",
        "                        monitor='val_categorical_accuracy',\n",
        "                        save_best_only=True,\n",
        "                        verbose=1)\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_categorical_accuracy',\n",
        "                           patience=10,\n",
        "                           restore_best_weights=True,\n",
        "                           mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n",
        "                              patience=3, min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "m-kOCOUQE8bP"
      },
      "cell_type": "code",
      "source": [
        "# First Pretraining the dense layer\n",
        "resnet50_history = resnet50_model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=N_STEPS,\n",
        "                             epochs=50,\n",
        "                             callbacks=[early_stop, checkpoint],\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=N_VAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "oQOtsi6vE8bR"
      },
      "cell_type": "code",
      "source": [
        "# re-train whole network end2end\n",
        "resnet50_model = compile_model('ResNet50', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n",
        "\n",
        "resnet50_model.load_weights('../working/model.weights.best.hdf5')\n",
        "\n",
        "train_generator.reset()\n",
        "test_generator.reset()\n",
        "\n",
        "resnet50_history = resnet50_model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=N_STEPS,\n",
        "                             epochs=N_EPOCHS,\n",
        "                             callbacks=[early_stop, checkpoint, reduce_lr],\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=N_VAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2Xyx0KcfE8bS"
      },
      "cell_type": "code",
      "source": [
        "plot_history(resnet50_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8GnvA4W1E8bS"
      },
      "cell_type": "code",
      "source": [
        "resnet50_model.load_weights('../working/model.weights.best.hdf5')\n",
        "\n",
        "class_indices = train_generator.class_indices\n",
        "class_indices = dict((v,k) for k,v in class_indices.items())\n",
        "\n",
        "test_generator_new = test_gen.flow_from_directory(\n",
        "    directory=TEST_DIR,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    seed=69\n",
        ")\n",
        "\n",
        "predictions = resnet50_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\n",
        "predicted_classes = np.argmax(np.rint(predictions), axis=1)\n",
        "true_classes = test_generator_new.classes\n",
        "\n",
        "prf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\n",
        "prf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8exxgjSDE8bS"
      },
      "cell_type": "code",
      "source": [
        "# Save the model and the weights\n",
        "resnet50_model.save('../working/ResNet50_eurosat.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwXav9JRE8bS"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.2 ResNet50V2 Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "m3n3eragE8bS"
      },
      "cell_type": "code",
      "source": [
        "resnet50V2_model = compile_model('ResNet50V2', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\n",
        "resnet50V2_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xVBuhUDHE8bS"
      },
      "cell_type": "code",
      "source": [
        "train_generator.reset()\n",
        "test_generator.reset()\n",
        "\n",
        "N_STEPS = train_generator.samples//BATCH_SIZE\n",
        "N_VAL_STEPS = test_generator.samples//BATCH_SIZE\n",
        "N_EPOCHS = 100\n",
        "\n",
        "# model callbacks\n",
        "checkpoint = ModelCheckpoint(filepath='../working/model.weights.best.hdf5',\n",
        "                        monitor='val_categorical_accuracy',\n",
        "                        save_best_only=True,\n",
        "                        verbose=1)\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_categorical_accuracy',\n",
        "                           patience=10,\n",
        "                           restore_best_weights=True,\n",
        "                           mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n",
        "                              patience=3, min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3ZsCrXxTE8bS"
      },
      "cell_type": "code",
      "source": [
        "# First Pretraining the dense layer\n",
        "resnet50V2_history = resnet50V2_model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=N_STEPS,\n",
        "                             epochs=50,\n",
        "                             callbacks=[early_stop, checkpoint],\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=N_VAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "l891ETwvE8bS"
      },
      "cell_type": "code",
      "source": [
        "# re-train whole network end2end\n",
        "resnet50V2_model = compile_model('ResNet50V2', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n",
        "\n",
        "resnet50V2_model.load_weights('../working/model.weights.best.hdf5')\n",
        "\n",
        "train_generator.reset()\n",
        "test_generator.reset()\n",
        "\n",
        "resnet50V2_history = resnet50V2_model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=N_STEPS,\n",
        "                             epochs=N_EPOCHS,\n",
        "                             callbacks=[early_stop, checkpoint, reduce_lr],\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=N_VAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "uX67dqXfE8bT"
      },
      "cell_type": "code",
      "source": [
        "plot_history(resnet50V2_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Qj3L24AbE8bT"
      },
      "cell_type": "code",
      "source": [
        "resnet50V2_model.load_weights('../working/model.weights.best.hdf5')\n",
        "\n",
        "class_indices = train_generator.class_indices\n",
        "class_indices = dict((v,k) for k,v in class_indices.items())\n",
        "\n",
        "test_generator_new = test_gen.flow_from_directory(\n",
        "    directory=TEST_DIR,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    seed=69\n",
        ")\n",
        "\n",
        "predictions = resnet50V2_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\n",
        "predicted_classes = np.argmax(np.rint(predictions), axis=1)\n",
        "true_classes = test_generator_new.classes\n",
        "\n",
        "prf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\n",
        "prf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "L5jdqn3fE8bT"
      },
      "cell_type": "code",
      "source": [
        "# Save the model and the weights\n",
        "resnet50V2_model.save('../working/ResNet50V2_eurosat.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hEoq4A6VE8bT"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.3 ResNet152V2 Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "T2tlopTZE8bT"
      },
      "cell_type": "code",
      "source": [
        "resnet152V2_model = compile_model('ResNet152V2', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\n",
        "resnet152V2_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yTYE-nHmE8bT"
      },
      "cell_type": "code",
      "source": [
        "train_generator.reset()\n",
        "test_generator.reset()\n",
        "\n",
        "N_STEPS = train_generator.samples//BATCH_SIZE\n",
        "N_VAL_STEPS = test_generator.samples//BATCH_SIZE\n",
        "N_EPOCHS = 100\n",
        "\n",
        "# model callbacks\n",
        "checkpoint = ModelCheckpoint(filepath='../working/model.weights.best.hdf5',\n",
        "                        monitor='val_categorical_accuracy',\n",
        "                        save_best_only=True,\n",
        "                        verbose=1)\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_categorical_accuracy',\n",
        "                           patience=10,\n",
        "                           restore_best_weights=True,\n",
        "                           mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n",
        "                              patience=3, min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AcHAup_fE8bT"
      },
      "cell_type": "code",
      "source": [
        "# First Pretraining the dense layer\n",
        "resnet152V2_history = resnet152V2_model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=N_STEPS,\n",
        "                             epochs=50,\n",
        "                             callbacks=[early_stop, checkpoint],\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=N_VAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Z7yuhRJBE8bT"
      },
      "cell_type": "code",
      "source": [
        "# re-train whole network end2end\n",
        "resnet152V2_model = compile_model('ResNet152V2', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n",
        "\n",
        "resnet152V2_model.load_weights('../working/model.weights.best.hdf5')\n",
        "\n",
        "train_generator.reset()\n",
        "test_generator.reset()\n",
        "\n",
        "resnet152V2_history = resnet152V2_model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=N_STEPS,\n",
        "                             epochs=N_EPOCHS,\n",
        "                             callbacks=[early_stop, checkpoint, reduce_lr],\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=N_VAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Q-A68QB-E8bT"
      },
      "cell_type": "code",
      "source": [
        "plot_history(resnet152V2_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "X0SDv--7E8bU"
      },
      "cell_type": "code",
      "source": [
        "resnet152V2_model.load_weights('../working/model.weights.best.hdf5')\n",
        "\n",
        "class_indices = train_generator.class_indices\n",
        "class_indices = dict((v,k) for k,v in class_indices.items())\n",
        "\n",
        "test_generator_new = test_gen.flow_from_directory(\n",
        "    directory=TEST_DIR,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    seed=69\n",
        ")\n",
        "\n",
        "predictions = resnet152V2_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\n",
        "predicted_classes = np.argmax(np.rint(predictions), axis=1)\n",
        "true_classes = test_generator_new.classes\n",
        "\n",
        "prf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\n",
        "prf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3Is7NYzvE8bU"
      },
      "cell_type": "code",
      "source": [
        "# Save the model and the weights\n",
        "resnet152V2_model.save('../working/ResNet152V2_eurosat.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gxGp4ifeE8bU"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.4 VGG16 Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Kuf_VnlpE8bU"
      },
      "cell_type": "code",
      "source": [
        "vgg16_model = compile_model('VGG16', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\n",
        "vgg16_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "UJcSrzJEE8bU"
      },
      "cell_type": "code",
      "source": [
        "train_generator.reset()\n",
        "test_generator.reset()\n",
        "\n",
        "N_STEPS = train_generator.samples//BATCH_SIZE\n",
        "N_VAL_STEPS = test_generator.samples//BATCH_SIZE\n",
        "N_EPOCHS = 100\n",
        "\n",
        "# model callbacks\n",
        "checkpoint = ModelCheckpoint(filepath='../working/model.weights.best.hdf5',\n",
        "                        monitor='val_categorical_accuracy',\n",
        "                        save_best_only=True,\n",
        "                        verbose=1)\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_categorical_accuracy',\n",
        "                           patience=10,\n",
        "                           restore_best_weights=True,\n",
        "                           mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n",
        "                              patience=3, min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XHAJtQr7E8bU"
      },
      "cell_type": "code",
      "source": [
        "train_generator.reset()\n",
        "# First Pretraining the dense layer\n",
        "vgg16_history = vgg16_model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=N_STEPS,\n",
        "                             epochs=50,\n",
        "                             callbacks=[early_stop, checkpoint],\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=N_VAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bmiOeZ3zE8bU"
      },
      "cell_type": "code",
      "source": [
        "# re-train whole network end2end\n",
        "vgg16_model = compile_model('VGG16', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n",
        "\n",
        "vgg16_model.load_weights('../working/model.weights.best.hdf5')\n",
        "\n",
        "train_generator.reset()\n",
        "test_generator.reset()\n",
        "\n",
        "vgg16_history = vgg16_model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=N_STEPS,\n",
        "                             epochs=N_EPOCHS,\n",
        "                             callbacks=[early_stop, checkpoint, reduce_lr],\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=N_VAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vmrqFrtSE8bU"
      },
      "cell_type": "code",
      "source": [
        "plot_history(vgg16_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fo6a5bbyE8bV"
      },
      "cell_type": "code",
      "source": [
        "vgg16_model.load_weights('../working/model.weights.best.hdf5')\n",
        "\n",
        "class_indices = train_generator.class_indices\n",
        "class_indices = dict((v,k) for k,v in class_indices.items())\n",
        "\n",
        "test_generator_new = test_gen.flow_from_directory(\n",
        "    directory=TEST_DIR,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    seed=69\n",
        ")\n",
        "\n",
        "predictions = vgg16_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\n",
        "predicted_classes = np.argmax(np.rint(predictions), axis=1)\n",
        "true_classes = test_generator_new.classes\n",
        "\n",
        "prf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\n",
        "prf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AvMiQ_bNE8bV"
      },
      "cell_type": "code",
      "source": [
        "# Save the model and the weights\n",
        "vgg16_model.save('../working/vgg16_eurosat.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Iuvr3i0E8bV"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.5 VGG19 Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ZY7m4DVYE8bV"
      },
      "cell_type": "code",
      "source": [
        "vgg19_model = compile_model('VGG19', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\n",
        "vgg19_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "cnvoOiMyE8bV"
      },
      "cell_type": "code",
      "source": [
        "train_generator.reset()\n",
        "test_generator.reset()\n",
        "\n",
        "N_STEPS = train_generator.samples//BATCH_SIZE\n",
        "N_VAL_STEPS = test_generator.samples//BATCH_SIZE\n",
        "N_EPOCHS = 100\n",
        "\n",
        "# model callbacks\n",
        "checkpoint = ModelCheckpoint(filepath='../working/model.weights.best.hdf5',\n",
        "                        monitor='val_categorical_accuracy',\n",
        "                        save_best_only=True,\n",
        "                        verbose=1)\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_categorical_accuracy',\n",
        "                           patience=10,\n",
        "                           restore_best_weights=True,\n",
        "                           mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n",
        "                              patience=3, min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "RCBxN3npE8bV"
      },
      "cell_type": "code",
      "source": [
        "train_generator.reset()\n",
        "# First Pretraining the dense layer\n",
        "vgg19_history = vgg19_model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=N_STEPS,\n",
        "                             epochs=50,\n",
        "                             callbacks=[early_stop, checkpoint],\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=N_VAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GY155F1hE8bV"
      },
      "cell_type": "code",
      "source": [
        "# re-train whole network end2end\n",
        "vgg19_model = compile_model('VGG19', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n",
        "\n",
        "vgg19_model.load_weights('../working/model.weights.best.hdf5')\n",
        "\n",
        "train_generator.reset()\n",
        "test_generator.reset()\n",
        "\n",
        "vgg19_history = vgg19_model.fit_generator(train_generator,\n",
        "                             steps_per_epoch=N_STEPS,\n",
        "                             epochs=N_EPOCHS,\n",
        "                             callbacks=[early_stop, checkpoint, reduce_lr],\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=N_VAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0vkDwbHTE8bV"
      },
      "cell_type": "code",
      "source": [
        "plot_history(vgg19_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7gHnYAUqE8bV"
      },
      "cell_type": "code",
      "source": [
        "vgg19_model.load_weights('../working/model.weights.best.hdf5')\n",
        "\n",
        "class_indices = train_generator.class_indices\n",
        "class_indices = dict((v,k) for k,v in class_indices.items())\n",
        "\n",
        "test_generator_new = test_gen.flow_from_directory(\n",
        "    directory=TEST_DIR,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    seed=69\n",
        ")\n",
        "\n",
        "predictions = vgg19_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\n",
        "predicted_classes = np.argmax(np.rint(predictions), axis=1)\n",
        "true_classes = test_generator_new.classes\n",
        "\n",
        "prf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\n",
        "prf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "W9plRHBwE8bV"
      },
      "cell_type": "code",
      "source": [
        "# Save the model and the weights\n",
        "vgg19_model.save('../working/vgg19_eurosat.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yp7xaGTqE8bW"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EjsjTY1jE8bW"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "V_qLj7bGE8bW"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VUV8IHrGE8bW"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zKdf-NXHE8bW"
      },
      "cell_type": "code",
      "source": [
        "plot_predictions(true_classes, predictions, test_generator_new, class_indices)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Land Cover Classification with EuroSAT Dataset",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}